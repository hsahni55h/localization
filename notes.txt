Localization Algorithms

1. Extented kalman filter - gaussian filter which helps in estimating state of non linear models

2. Markov Localization - Baye's filter localization algorithm, maintains a probability distribution over the set of
                        all possble positions and orientations where the robot might be located at.

3. Grid localization - histogram filter, since it is capable of estimating robots positions using grids

4. Monte Carlo Localization - particle filter, it estimates robot location using particles 

-------------------------------------------------------------------------------------------------------------
Localization Challenges

1. position tracking ( local localization) -robot knows its initial pose but the challenge entails in estimating pose 
                                            when it moves around in the environment.

2. global localization - initial pose is unknown and it must estimate its pose relative to the ground truth

3. kidnapped robot - robot may be moved to a new point in the map at any time 
-------------------------------------------------------------------------------------------------------------
Kalman Filter types

1. KF - linear
2. EKF - non linear
3. UKF - highly non linear

-------------------------------------------------------------------------------------------------------------
1D gaussian

At the basis of the Kalman Filter is the Gaussian distribution, sometimes referred to as a bell curve or normal distribution. 
Recall the rover example - after executing one motion, the rover’s location was represented by a Gaussian. 
It’s exact location was not certain, but the level of uncertainty was bounded. 
This is the role of a Kalman Filter - after a movement or a measurement update, it outputs a unimodal Gaussian distribution. 
This is its best guess at the true value of a parameter.

A Gaussian distribution is a probability distribution, which is a continuous function. 
The probability that a random variable, x, will take a value between x1 and x2 is given by the integral of the function from x1 to x2.

p (x1 < x < x2) = ∫ f(x)dx (integral from x1 to x2)


Mean and Variance

A Gaussian is characterized by two parameters - its mean (μ) and its variance (σ²). 
The mean is the most probable occurrence and lies at the centre of the function, 
and the variance relates to the width of the curve. 

The term unimodal implies a single peak present in the distribution.

Gaussian distributions are frequently abbreviated as N(x: μ, σ²), and will be referred to in this way.


The formula for the Gaussian distribution is mentioned below. 

p(x) = (1 / σ * sqrt(2π)) e^(-(x−μ)^2)/2*σ^2

The formula contains an exponential of a quadratic function. 
The quadratic compares the value of x to μ, and in the case that x=μ, the exponential is equal to 1 (e^0 =1)
The constant in front of the exponential is a necessary normalizing factor.

Just like with discrete probability, like a coin toss, the probabilities of all the options must sum to one. 
Therefore, the area underneath the function always sums to one.
∫p(x)dx=1

-------------------------------------------------------------------------------------------------------------

Variable naming conventions

xt : state (x, y, theta)
zt : measurement (from sensors)
ut : control action


-------------------------------------------------------------------------------------------------------------
1D kalman filter

-----------------------------------------
Measurement update:
-----------------------------------------
mu: Mean of the prior belief
sigma^{2}: Variance of the prior belief  (uncertainty of prior belief)

v: Mean of the measurement
r^{2}: Variance of the measurement (uncertainty of the measurement)

The new mean is a weighted sum of the prior belief and measurement means. 
With uncertainty, a larger number represents a more uncertain probability distribution. 
However, the new mean should be biased towards the measurement update, which has a smaller standard deviation than the prior.

μ' = (r^{2}*mu + sigma^{2}*v) / (r^{2} + sigma^{2})


The uncertainty of the prior is multiplied by the mean of the measurement, to give it more weight, 
and similarly the uncertainty of the measurement is multiplied with the mean of the prior.


Next, we need to determine the variance of the new state estimate.
The two Gaussians provide us with more information together than either Gaussian offered alone. 
As a result, our new state estimate is more confident than our prior belief and our measurement. 
This means that it has a higher peak and is narrower. 

sigma'^{2} = 1/(1/r^{2} + 1/sigma^{2})

mu: Mean of the prior belief
sigma^{2}: Variance of the prior belief     (uncertainty of prior belief)

v: Mean of the measurement
r^{2}: Variance of the measurement      (uncertainty of the measurement)

tau: Mean of the posterior
s^{2}: Variance of the posterior


-----------------------------------------
State prediction
-----------------------------------------




-------------------------------------------------------------------------------------------------------------



-------------------------------------------------------------------------------------------------------------