Localization Algorithms

1. Extented kalman filter - gaussian filter which helps in estimating state of non linear models

2. Markov Localization - Baye's filter localization algorithm, maintains a probability distribution over the set of
                        all possble positions and orientations where the robot might be located at.

3. Grid localization - histogram filter, since it is capable of estimating robots positions using grids

4. Monte Carlo Localization - particle filter, it estimates robot location using particles 

-------------------------------------------------------------------------------------------------------------
Localization Challenges

1. position tracking ( local localization) -robot knows its initial pose but the challenge entails in estimating pose 
                                            when it moves around in the environment.

2. global localization - initial pose is unknown and it must estimate its pose relative to the ground truth

3. kidnapped robot - robot may be moved to a new point in the map at any time 
-------------------------------------------------------------------------------------------------------------
Kalman Filter types

1. KF - linear
2. EKF - non linear
3. UKF - highly non linear

-------------------------------------------------------------------------------------------------------------
1D gaussian

At the basis of the Kalman Filter is the Gaussian distribution, sometimes referred to as a bell curve or normal distribution. 
Recall the rover example - after executing one motion, the rover’s location was represented by a Gaussian. 
It’s exact location was not certain, but the level of uncertainty was bounded. 
This is the role of a Kalman Filter - after a movement or a measurement update, it outputs a unimodal Gaussian distribution. 
This is its best guess at the true value of a parameter.

A Gaussian distribution is a probability distribution, which is a continuous function. 
The probability that a random variable, x, will take a value between x1 and x2 is given by the integral of the function from x1 to x2.

p (x1 < x < x2) = ∫ f(x)dx (integral from x1 to x2)


Mean and Variance

A Gaussian is characterized by two parameters - its mean (μ) and its variance (σ²). 
The mean is the most probable occurrence and lies at the centre of the function, 
and the variance relates to the width of the curve. 

The term unimodal implies a single peak present in the distribution.

Gaussian distributions are frequently abbreviated as N(x: μ, σ²), and will be referred to in this way.


The formula for the Gaussian distribution is mentioned below. 

p(x) = (1 / σ * sqrt(2π)) e^(-(x−μ)^2)/2*σ^2

The formula contains an exponential of a quadratic function. 
The quadratic compares the value of x to μ, and in the case that x=μ, the exponential is equal to 1 (e^0 =1)
The constant in front of the exponential is a necessary normalizing factor.

Just like with discrete probability, like a coin toss, the probabilities of all the options must sum to one. 
Therefore, the area underneath the function always sums to one.
∫p(x)dx=1

-------------------------------------------------------------------------------------------------------------

Variable naming conventions

xt : state (x, y, theta)
zt : measurement (from sensors)
ut : control action


-------------------------------------------------------------------------------------------------------------
1D kalman filter

-----------------------------------------
Measurement update:
-----------------------------------------
mu: Mean of the prior belief
sigma^{2}: Variance of the prior belief  (uncertainty of prior belief)

v: Mean of the measurement
r^{2}: Variance of the measurement (uncertainty of the measurement)

The new mean is a weighted sum of the prior belief and measurement means. 
With uncertainty, a larger number represents a more uncertain probability distribution. 
However, the new mean should be biased towards the measurement update, which has a smaller standard deviation than the prior.

μ' = (r^{2}*mu + sigma^{2}*v) / (r^{2} + sigma^{2})


The uncertainty of the prior is multiplied by the mean of the measurement, to give it more weight, 
and similarly the uncertainty of the measurement is multiplied with the mean of the prior.


Next, we need to determine the variance of the new state estimate.
The two Gaussians provide us with more information together than either Gaussian offered alone. 
As a result, our new state estimate is more confident than our prior belief and our measurement. 
This means that it has a higher peak and is narrower. 

sigma'^{2} = 1/(1/r^{2} + 1/sigma^{2})

mu: Mean of the prior belief
sigma^{2}: Variance of the prior belief     (uncertainty of prior belief)

v: Mean of the measurement
r^{2}: Variance of the measurement      (uncertainty of the measurement)

tau: Mean of the posterior
s^{2}: Variance of the posterior


-----------------------------------------
State prediction
-----------------------------------------
Posterior Mean: μ' = μ1 + μ2
Posterior Variance: σ'^2 = σ1^2 + σ2^2

-------------------------------------------------------------------------------------------------------------
Formulas for the Multivariate Gaussian

The mean is now a vector,

μ = [ μx
    μy ]

And the multidimensional equivalent of variance is a covariance matrix,

Σ = [ σx^2      σyσx​
      σxσy      σy^2 ]

Where σx^2 and σy^2 represent the variances, while σyσx and σxσy are correlation terms.
These terms are non-zero if there is a correlation between the variance in one dimension and the variance in another. 
When that is the case, the Gaussian function looks 'skewed' when looked at from above.
If we were to evaluate this mathematically, the eigenvalues and eigenvectors of the covariance matrix describe the amount and direction of uncertainty.


Multivariate Gaussian
Below is the formula for the multivariate Gaussian. Note that x and μ are vectors, and Σ is a matrix.

# check the formula in the image

-------------------------------------------------------------------------------------------------------------

-----------------------------------------
Design of Multi-Dimensional Kalman Filters
-----------------------------------------

State Transition
-----------------

The formula below is the state transition function that advances the state from time t to time t + 1. 
It is just the relationship between the robot’s position, x, and velocity, x(dot). 
Here, we will assume that the robot’s velocity is not changing.

x' = x + Δtx(dot)
x'(dot) = x(dot)

We can express the same relationship in matrix form, as seen below. 
On the left, is the posterior state (denoted with the prime symbol, '), 
and on the right are the state transition function and the prior state. 
This equation shows how the state changes over the time period, Δt.
Note that we are only working with the means here; the covariance matrix will appear later.


|   x    |   =   | 1   Δt |  |  x   |
| x(dot) |'      | 0    1 |  |x(dot)|


The State Transition Function is denoted  by F, and the formula can be written as so,

x' = Fx
In reality, the equation should also account for process noise, as its own term in the equation. 
However, process noise is a Gaussian with a mean of 0, so the update equation for the mean need not include it.

x' = Fx + noise
noise∼N(0,Q)

Sidenote: While it is common to use Σ to represent the covariance of a Gaussian distribution in mathematics, 
it is more common to use the letter P to represent the state covariance in localization.

If you multiply the state, x, by F, then the covariance will be affected by the square of F. 
In matrix form, this will look like so:

P' = FPF^T (transpose)
 
However, your intuition may suggest that it should be affected by more than just the state transition function. 
For instance, additional uncertainty may arise from the prediction itself.

To calculate the posterior covariance, the prior covariance is multiplied by the state transition function squared, and 
Q added as an increase of uncertainty due to process noise. 
Q can account for a robot slowing down unexpectedly, or being drawn off course by an external influence.

P' = FPF^T + Q
Now we’ve updated the mean and the covariance as part of the state prediction.
 
 
Measurement Update
-------------------
Next, we move onto the measurement update step. 
If we return to our original example, where we were tracking the position and velocity of a robot in the x-dimension, 
the robot was taking measurements of the location only (the velocity is a hidden state variable). 
Therefore the measurement function is very simple - a matrix containing a one and a zero. 
This matrix demonstrates how to map the state to the observation, 

z = [ 1 0] [ x
            x(dot)]
This matrix, called the Measurement Function, is denoted H.


For the measurement update step, there are a few formulas. 
First, we calculate the measurement residual, y. 
The measurement residual is the difference between the measurement and the expected measurement based on the prediction 
(ie. we are comparing where the measurement tells us we are vs. where we think we are). 
The measurement residual will be used later on in a formula.

y = z - Hx'

Next, it's time to consider the measurement noise, denoted R.
This formula maps the state prediction covariance into the measurement space and adds the measurement noise. 
The result, S, will be used in a subsequent equation to calculate the Kalman Gain.

S = HP'H^T + R



Kalman Gain
------------

Next, we calculate the Kalman Gain, K. 
As you will see in the next equation, the Kalman Gain determines how much weight should be placed on the state prediction, and how much on the measurement update. 
It is an averaging factor that changes depending on the uncertainty of the state prediction and measurement update.

K = P'(H^T)(S^-1)
x = x' + Ky


The last step in the Kalman Filter is to update the new state’s covariance using the Kalman Gain.
P = (I - KH)P'

-------------------------------------------------------------------------------------------------------------


KALMAN FILTER EQUATIONS


State Prediction:

x' = Fx
P' = FPF^T + Q

Measurement Update:

y = z - Hx'
S = HP'H^T + R


Calculation of Kalman Gain:

K = P'(H^T)(S^-1)

Calculation of Posterior State and Covariance:

x = x' + Ky
P = (I - KH)P'

The Kalman Filter can successfully recover from inaccurate initial estimates, but it is very important to estimate the noise parameters, Q and R, 
as accurately as possible - as they are used to determine which of the estimate or the measurement to believe more.



-------------------------------------------------------------------------------------------------------------

-----------------------
EXTENDED KALMAN FILTER
-----------------------

The Kalman Filter is applicable to problems with linear motion and measurement functions. 
This is limiting, as much of the real world is nonlinear.

A nonlinear function can be used to update the mean of a function, but not the variance, 
as this would result in a non-Gaussian distribution which is much more computationally expensive to work with. 
To update the variance, the Extended Kalman Filter linearizes the nonlinear function f(x) over a small section and calls it F. 
This linearization, F, is then used to update the state's variance.
The linear approximation can be obtained by using the first two terms of the Taylor Series of the function centered around the mean.

Now you’ve seen the fundamentals behind the Extended Kalman Filter. The mechanics are not too different from the Kalman Filter, 
with the exception of needing to linearize a nonlinear motion or measurement function to be able to update the variance.



##### Check the image linearization in multiple dimnsions #####

This will make more sense in context, so let’s look at a specific example. 
Let’s say that we are tracking the x-y coordinate of an object. 
This is to say that our state is a vector x, with state variables x and y.

x=[ x
    y ]
However, our sensor does not allow us to measure the x and y coordinates of the object directly. 
Instead, our sensor measures the distance from the robot to the object, r, as well as the angle between r and the x-axis, θ.

z = [ r
      θ ]
It is important to notice that our state is using a Cartesian representation of the world, 
while the measurements are in a polar representation. 
How will this affect our measurement function?

Our measurement function maps the state to the observation, as so,

[ x       -------->   [ r
  y ]     -------->     θ ]
Thus, our measurement function must map from Cartesian to polar coordinates.


But there is no matrix, H, that will successfully make this conversion, 
as the relationship between Cartesian and polar coordinates is nonlinear.

r = sqrt(x^2 + y^2)
θ = arctan(y/x)

For this reason, instead of using the measurement residual equation 

y = z − Hx'

that you had seen before, the mapping must be made with a dedicated function, h(x').

h(x') = [ sqrt(x^2 + y^2)
          arctan(y/x)     ]


Then the measurement residual equation becomes y = z − h(x').

Our measurement covariance matrix cannot be updated the same way, as it would turn into a non-Gaussian distribution. 
Let's calculate a linearization, H, and use it instead. 

The Taylor series for the function h(x), centered about the mean μ, is defined below.
h(x) ≃ h(μ) + (x−μ).T Df(μ)

The Jacobian, Df(μ), is defined below. But let's call it H since it's the linearization of our measurement function, h(x).

H = ⎡   ∂r/∂x   ∂r/∂y   ⎤
    ⎣   ∂θ/∂x   ∂θ/∂y   ⎦


If you were to compute each of those partial derivatives, the matrix would reduce to the following,


H = ⎡   x/sqrt(x^2 + y^2)       y/sqrt(x^2 + y^2)   ⎤
    ⎢                                               ⎥
    ⎣   -y/(x^2 + y^2)          x/(x^2 + y^2)       ⎦


It's this matrix, H, that can then be used to update the state's covariance.



---------------------------------
​Extended Kalman Filter Equations
---------------------------------

State Prediction:

x' = f(x)
P' = FPF^T + Q

Measurement Update:

y = z − h(x')
S = HP'H^T + R

Calculation of Kalman Gain:

K = P'(H^T)(S^-1)

Calculation of Posterior State and Covariance:

x = x' + Ky
P = (I - KH)P'
  
The Extended Kalman Filter requires us to calculate the Jacobian of a nonlinear function as part of every single iteration, 
since the mean (which is the point that we linearize about) is updated.


The key take-aways about Extended Kalman Filters:

The Kalman Filter cannot be used when the measurement and/or state transition functions are nonlinear, 
since this would result in a non-Gaussian distribution.

Instead, we take a local linear approximation and use this approximation to update the covariance of the estimate. 
The linear approximation is made using the first terms of the Taylor Series, which includes the first derivative of the function.

In the multi-dimensional case, taking the first derivative isn't as easy as there are multiple state variables and multiple dimensions. 
Here we employ a Jacobian, which is a matrix of partial derivatives, containing the partial derivative of each dimension with respect to each state variable.


​